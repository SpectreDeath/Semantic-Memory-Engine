# Layer 0 Decision Guide: Spider vs Crawler

## Quick Decision Tree

```
Do you need to:
â”œâ”€ Capture single/batch URLs?
â”‚  â””â”€ YES â†’ Use HarvesterSpider âœ…
â”‚
â”œâ”€ Crawl domain recursively?
â”‚  â””â”€ YES â†’ Use HarvesterCrawler (deep_crawl_domain)
â”‚
â”œâ”€ Extract tables/forms/JSON-LD?
â”‚  â””â”€ YES â†’ Use HarvesterCrawler (extract_structured_data)
â”‚
â””â”€ Handle complex SPA with custom JS?
   â””â”€ YES â†’ Use HarvesterCrawler (bypass_dynamic_content)

Default for Stage 1 (Semantic Compression): HarvesterSpider âœ…
```

---

## Architecture Comparison

### HarvesterSpider (Focused, Fast)

```
Philosophy: Minimal, async-first, memory-efficient

Architecture:
    URL
     â†“
    AsyncWebCrawler (Crawl4AI)
     â”œâ”€ Headless browser
     â”œâ”€ PruningContentFilter (remove noise)
     â””â”€ fit_markdown extraction
     â†“
    Centrifuge DB (raw_content table)
     â”œâ”€ markdown_content (LLM-ready)
     â”œâ”€ processed_by_loom = False
     â””â”€ source_quality (0-100)
     â†“
    [Ready for Loom]

Code Size: ~300 lines (core logic)
Dependencies: crawl4ai, asyncio, sqlite3
Memory: <200MB overhead
GPU: 0% (free for Loom)

Main Methods:
- capture_site(url) â†’ (capture_id, markdown)
- batch_capture(urls) â†’ results
- get_unprocessed_content(limit) â†’ pending items
- mark_processed(url) â†’ update DB
- get_stats() â†’ database stats
```

### HarvesterCrawler (Comprehensive, Flexible)

```
Philosophy: Complete toolkit with fallbacks

Architecture:
    â”œâ”€ Engine 1: Crawl4AI (primary)
    â”œâ”€ Engine 2: Scrapling (fallback)
    â”œâ”€ Engine 3: Playwright (complex SPA)
    â””â”€ Engine 4: BeautifulSoup (static HTML)
     â†“
    Feature Modules:
    â”œâ”€ fetch_semantic_markdown() - single URL
    â”œâ”€ deep_crawl_domain() - recursive BFS
    â”œâ”€ extract_structured_data() - tables/forms/JSON
    â”œâ”€ bypass_dynamic_content() - SPA/JS handling
    â””â”€ archive_to_centrifuge() - DB storage
     â†“
    Centrifuge DB (raw_content table)

Code Size: ~600 lines + helpers
Dependencies: crawl4ai, scrapling, playwright, beautifulsoup4
Memory: 500MB-2GB (parallel workers)
GPU: 0% (free for Loom)

Methods:
- fetch_semantic_markdown(url, js_render, wait_for)
- deep_crawl_domain(seed_url, max_depth, max_pages, parallel_workers)
- extract_structured_data(url, schema_hints)
- bypass_dynamic_content(url, wait_selector, scroll, interaction_script)
- archive_to_centrifuge(url, overwrite)
```

---

## Use Case Matrix

| Scenario | Spider | Crawler | Recommendation |
|----------|--------|---------|-----------------|
| **Single URL fetch** | 150-300ms | 150-300ms | Spider (simpler) |
| **Batch 10 URLs** | 8-12s (async) | 10-15s | Spider (faster) |
| **Recursive domain crawl** | N/A | 20-25s (20 pages) | Crawler |
| **Extract tables** | N/A | 500-800ms | Crawler |
| **Complex React SPA** | Limited | Optimized | Crawler |
| **Stage 1 ingestion** | âœ… Perfect | Overkill | Spider |
| **Research mode** | Limited | âœ… Complete | Crawler |

---

## Performance Benchmark

### Spider (Single URL, Sequential)

```
Static page (Wikipedia):
  â”œâ”€ Load: 150-300ms
  â”œâ”€ Parse: <100ms
  â””â”€ Archive: <50ms
  = 200-350ms total
  = 2-3 pages/sec

Dynamic page (React app):
  â”œâ”€ Load + JS: 2-5s
  â”œâ”€ Parse: <100ms
  â””â”€ Archive: <50ms
  = 2-5.1s total
  = 0.2-0.5 pages/sec
```

### Spider (Batch, Concurrent)

```
10 URLs, async:
  â”œâ”€ Max concurrent: 4 (system resources)
  â”œâ”€ Mixed static/dynamic
  â”œâ”€ Effective: 8-12s
  = 0.8-1.2 pages/sec
  = 50-70 pages/minute

vs Sequential 10 URLs:
  â”œâ”€ Time: 25-40 seconds
  = 0.25-0.4 pages/sec
  
Speedup: 3-4x âœ…
```

### Crawler (Recursive Domain)

```
20 pages, 1 worker:
  = 60-80 seconds
  = 0.25-0.33 pages/sec

20 pages, 4 workers:
  = 20-25 seconds
  = 0.8-1.0 pages/sec
  
Parallelization: 3-4x faster
```

---

## Memory Profile

### Spider
```
Baseline: ~100MB (Python + sqlite3)
Per concurrent request: +50MB
With 4 concurrent: ~300MB total
On 32GB: âœ… Negligible (0.9%)
```

### Crawler
```
Baseline: ~150MB (includes all engines)
Per parallel worker: +200MB
With 4 workers: ~950MB total
On 32GB: âœ… Acceptable (3%)
```

---

## Real-World Workflows

### Workflow 1: Stage 1 Semantic Compression

```
Use: HarvesterSpider (optimal)

â†’ Scout identifies knowledge gap
â†’ Get best source URL
â†’ Spider.capture_site(url)
â†’ Archive to raw_content
â†’ Loom processes (async)
â†’ Synapse consolidates

Time: 5-30 seconds total
Code: ~20 lines
```

### Workflow 2: Domain Research

```
Use: HarvesterCrawler (required)

â†’ User needs comprehensive domain knowledge
â†’ Crawler.deep_crawl_domain(seed_url, max_pages=50)
â†’ Batch archive all URLs
â†’ Loom processes 50 pages async
â†’ Results in Beacon dashboard

Time: 2-5 minutes
Code: ~10 lines
```

### Workflow 3: Table Extraction

```
Use: HarvesterCrawler (required)

â†’ Need structured data from page
â†’ Crawler.extract_structured_data(url)
â†’ Tables returned as CSV-like rows
â†’ Convert to atomic facts manually
â†’ Archive to Centrifuge

Time: <1 second
Code: ~5 lines
```

### Workflow 4: SPA/JavaScript

```
Use: HarvesterCrawler (required)

â†’ Target is React/Vue/Angular app
â†’ Crawler.bypass_dynamic_content(url, wait_selector='.loaded')
â†’ Custom JS execution if needed
â†’ Full rendered content extracted
â†’ Archive to raw_content

Time: 3-10 seconds
Code: ~5 lines
```

---

## Integration Patterns

### Pattern 1: Simple Ingestion (Spider)

```python
import asyncio
from harvester_spider import HarvesterSpider

async def ingest_single_url(url):
    spider = HarvesterSpider()
    capture_id, markdown = await spider.capture_site(url)
    return capture_id

result = asyncio.run(ingest_single_url("https://..."))
```

**When:** Scout gap â†’ single best source
**Time:** 2-5 seconds
**Lines:** 7

---

### Pattern 2: Batch Ingestion (Spider)

```python
import asyncio
from harvester_spider import HarvesterSpider

async def ingest_batch(urls):
    spider = HarvesterSpider()
    results = await spider.batch_capture(urls)
    return results

results = asyncio.run(ingest_batch([url1, url2, url3]))
```

**When:** Multiple sources, parallel processing
**Time:** 8-15 seconds (10 URLs)
**Lines:** 8

---

### Pattern 3: Domain Crawl (Crawler Required)

```python
from harvester_crawler import HarvesterCrawler

def crawl_domain(seed_url):
    crawler = HarvesterCrawler()
    result = crawler.deep_crawl_domain(
        seed_url, 
        max_depth=2, 
        max_pages=50
    )
    return result

result = crawl_domain("https://...")
```

**When:** Comprehensive domain knowledge needed
**Time:** 20-120 seconds
**Lines:** 8

---

### Pattern 4: Hybrid (Scout + Spider + Crawler)

```python
from adaptive_scout import AdaptiveScout
from harvester_spider import HarvesterSpider
from harvester_crawler import HarvesterCrawler
import asyncio

async def intelligent_harvest(query):
    scout = AdaptiveScout()
    spider = HarvesterSpider()
    
    complexity = scout.estimate_query_complexity(query)
    
    if complexity >= 8:
        # Deep crawl needed
        crawler = HarvesterCrawler()
        seed_url = scout.find_best_source(query)
        result = crawler.deep_crawl_domain(seed_url, max_pages=100)
        return result
    else:
        # Simple capture
        seed_url = scout.find_best_source(query)
        capture_id, _ = await spider.capture_site(seed_url)
        return capture_id

result = asyncio.run(intelligent_harvest("latest AI safety"))
```

**When:** Adaptive query-driven ingestion
**Time:** 5-120 seconds (based on complexity)
**Lines:** 25

---

## Recommendation Matrix

### For SimpleMem Stage 1 (Semantic Compression)

**Primary:** ğŸŸ¢ HarvesterSpider
- Simple, fast, memory-efficient
- Optimized for single/batch URL ingestion
- Clean Markdown â†’ Loom pipeline
- Async for non-blocking operations

**Secondary:** ğŸŸ¡ HarvesterCrawler
- Use only for advanced scenarios:
  - Recursive domain crawling (knowledge base building)
  - Structured data extraction (tables/forms)
  - Complex SPA handling
  - Batch domain analysis

---

## Migration Guide

### If You Have Existing Crawler Code

```python
# Old approach (comprehensive but heavy):
from harvester_crawler import HarvesterCrawler
crawler = HarvesterCrawler()
result = crawler.fetch_semantic_markdown("https://...")

# New approach (simpler, faster):
from harvester_spider import HarvesterSpider
import asyncio

spider = HarvesterSpider()
capture_id, markdown = asyncio.run(spider.capture_site("https://..."))
```

### To Use Both (Recommended)

```python
# For most operations: Spider
from harvester_spider import HarvesterSpider
spider = HarvesterSpider()

# For advanced needs: Crawler
from harvester_crawler import HarvesterCrawler
crawler = HarvesterCrawler()

# Use Spider by default, Crawler when needed
if is_complex_task:
    result = crawler.deep_crawl_domain(...)
else:
    result = asyncio.run(spider.capture_site(...))
```

---

## Configuration Presets

### Preset 1: Fast Ingestion (Default)

```python
from harvester_spider import HarvesterSpider

spider = HarvesterSpider()  # Uses defaults:
# - PruningContentFilter(threshold=0.45, min_word_count=50)
# - word_count_threshold=100
# - cache_mode="BYPASS"

# Result: 2-5s per page, quality ~85/100
```

### Preset 2: High Quality (Stricter)

```python
from harvester_spider import HarvesterSpider
from crawl4ai.content_filter_strategy import PruningContentFilter

spider = HarvesterSpider()
spider.content_filter = PruningContentFilter(
    threshold=0.60,      # More aggressive filtering
    min_word_count=100
)
spider.run_config.word_count_threshold = 200  # Stricter

# Result: 2-5s per page, quality ~95/100, less content
```

### Preset 3: Maximum Coverage (Looser)

```python
from harvester_spider import HarvesterSpider
from crawl4ai.content_filter_strategy import PruningContentFilter

spider = HarvesterSpider()
spider.content_filter = PruningContentFilter(
    threshold=0.30,      # Less aggressive
    min_word_count=20
)
spider.run_config.word_count_threshold = 50  # Looser

# Result: 2-5s per page, quality ~70/100, more content
```

---

## File Organization

```
d:\mcp_servers\
â”œâ”€â”€ harvester_spider.py              â† Use this (Stage 1)
â”œâ”€â”€ harvester_crawler.py             â† Use for advanced
â”œâ”€â”€ SPIDER_INTEGRATION_GUIDE.md       â† Start here
â”œâ”€â”€ HARVESTER_INTEGRATION_GUIDE.md    â† Advanced
â””â”€â”€ ARCHITECTURE_LAYER0_HARVESTER.md â† Reference
```

---

## Summary

| Aspect | Spider | Crawler |
|--------|--------|---------|
| **Lines of code** | ~300 | ~600 |
| **Setup complexity** | Simple | Moderate |
| **Single URL** | âœ… Optimal | Compatible |
| **Batch URLs** | âœ… Optimal | Works |
| **Domain crawl** | N/A | âœ… Optimal |
| **Struct extraction** | N/A | âœ… Optimal |
| **SPA handling** | Limited | âœ… Robust |
| **Memory** | <300MB | <1GB |
| **Recommended for** | Stage 1 | Advanced |

---

**Default Choice:** ğŸ•¸ï¸ **HarvesterSpider** for Stage 1 Semantic Compression
**Advanced Needs:** ğŸ”§ **HarvesterCrawler** when required

Both available, use what fits your current workflow!
